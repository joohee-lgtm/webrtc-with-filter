## 실시간 캔버스 랜더링과 필터
- MediaStream 으로 비디오가 재생될때, 캔버스도 함께 업데이트 해준다.
- 디바이스 접근 이후에는 모든 것이 화면에 보여지는 canvas 를 중심으로 구현된다.
- 비디오를 매번 렌더링 해주면서 필터 렌더링도 함께 진행하면 requestFrameAnimation warning 을 자주 볼 수 있지만 ... 렌더링 성능의 개선 포인트로 삼아보도록 하자 ..

### `CanvasRenderingContext2D.filter` 로 색상 필터 적용하기
- 흑백처리, 블러, 대비 같은 이미지 효과를 줄 수 있다. 
- css 필터 속성과 동일한 값을 지정할 수 있다.
- svg 필터를 `url(#svg_id)` 와 같은 형태로 입력할 수 있다.
    - Sketch 에서는 작업물을 svg 코드로 복사할 수 있다. [참고](https://oss.navercorp.com/ApolloFE/realtime-filter/issues/3#issuecomment-2695255)
- 캔버스에서 제공하는 2d context API 를 사용하기 때문에 특정 부분에 대해서만 필터를 적용하거나, 텍스트를 추가하는 등의 추가적인 효과도 적용할 수 있다.
- 화면 중앙에 블러 효과를 추가한 코드 예시

<img width="300" alt="2018-11-24 9 49 08" src="https://media.oss.navercorp.com/user/237/files/9f3fbc46-f4c1-11e8-8dfa-af0d10587258">

```javascript
function render() {
  const bufferContext = buffer.getContext('2d');
  const outputContext =  output.getContext('2d');
  let width, height;

  // 거울모드
  width = buffer.width = video.videoWidth;
  heigth = buffer.height = video.videoHeight;
  bufferContext.translate(video.videoWidth, 0);
  bufferContext.scale(-1, 1);

  // 원본 스트림 렌더링 (배경)
  bufferContext.filter = ""
  bufferContext.drawImage(video, 0, 0);
  outputContext.drawImage(buffer,0,0);
  
  // 블러 영역 렌더링
  const x = buffer.width/2 - (buffer.width/4);
  const y = buffer.height/2 - (buffer.height/4);
  bufferContext.filter = "blur(20px)";
  bufferContext.drawImage(video, 0, 0);
  outputContext.drawImage(buffer, x, y, width/2, height/2, x, y, width/2, height/2);
  window.requestAnimationFrame(render);
}
```

### webgl 필터 적용하기
- webgl 필터는 opengl 언어를 사용하는 shader 와 webgl 컨텍스트를 함께 사용함으로써 구현할 수 있다.
- 하지만 .. 쉐이더 코드를 잘 모를때는 gifx.js 과 같은 라이브러리를 사용해서 적용할 수 있다.
- gifx 는 webgl 컨텍스트 캔버스를 사용하기 때문에 필터 위에 바로 텍스트나 도형을 추가하는 2d context API 사용은 불가능하다.
   - webgl context 와 2d context 는 함께 사용이 불가능 하다.
   - 모든 캔버스는 첫번째 호출되는 컨텍스트로 캔버스의 컨텍스트가 고정된다.
- 필터가 씌워진 webgl context canvas 와 글씨나 도형을 (상대적으로) 쉽게 그릴 수 있는 2d context canvas 를 병합하여 사용할 수 있다.
- 아래는 webgl 과 shader 로 만들어진 gifx.js 라이브러리를 사용하여 필터를 적용하고 2d context fillText와 stroke API 를 이용해서 적용한 예시이다.

<img width="300" alt="2018-11-24 9 49 08" src="https://media.oss.navercorp.com/user/237/files/266aa23a-f4cc-11e8-9879-4ed91f9a3314">


```javascript
function render(){
    // webgl context <canvas> element
    // 일반 canvas 엘리먼트 아님, blur, draw 등의 라이브러리 API 가 바인딩 되어있다.
    const glfxCanvas = glfx.canvas(); 
    // MediaStream 이 재생되고 있는 <video> 를 webgl texture 로 사용
    const videoTexture = glfxCanvas.texture(video); 
    // glfx 라이브러리로 hexagon 효과 적용
    glfxCanvas.draw(videoTexture).hexagonalPixelate(320, 239.5, 20).update();
    // 2d 캔버스에 webgl 캔버스를 렌더링한다
    outputContext.drawImage(glfxCanvas, 0, 0);
    // 2d context api 사용하여 도형 추가
    const text = "WebGL & WebRTC";
    const textWidth = outputContext.measureText(text).width;
    ... style text 코드 생략
    outputContext.fillText(text,buffer.width/2,buffer.height/2);
    outputContext.beginPath();
    outputContext.moveTo(buffer.width/2-textWidth/2-30, buffer.height/2-50);
    ... lineTo 코드 생략
    outputContext.stroke();
    window.requestAnimationFrame(render);
}
```

### fabric.js 로 인터렉션 추가하기
- TODO


### face tracking 필터 적용하기
#### clmtrackr
- 얼굴 각 요소에 대한 정확한 x,y 좌표를 배열형태로 리턴해준다.
    - `[[x1, y1], [x2, y2]...[x70, y70]]`
- 가이드와 매핑되는 인덱스에서 각각의 얼굴 요소를 추출해서 사용할 수 있다. [추출하는 코드](https://oss.navercorp.com/ApolloFE/realtime-filter/blob/master/src/demos/facetracking-clmtrackr/index.js#L37)
   - 사각형 범위가 아닌, 정확한 shape 을 표현 할 수 있는 자료이기 때문에 표정 분석 [데모](https://www.auduno.com/clmtrackr/examples/clm_emotiondetection.html) / [예시코드](https://github.com/auduno/clmtrackr/blob/dev/examples/js/emotion_classifier.js) 와 같이 사용할 수 있다.
- 정규화된 모델링(?) 자료를 바탕으로 얼굴을 분석한다.
    - 본인의 얼굴 모델을 빌드해서 라이브러리에서 활용할 수 있다.
- 한명만 인식한다.

#### jeelizFaceFilter
- getUserDevice 접근에 관한 제어도 모두 라이브러리에서 처리하고 있다.
- three.js helper 를 따로 제공한다. `THREE.JeelizHelper.init(spec, detect_callback);`
- face tracking  과 head tracking 에 대한 데이터가 동시에 제공되기 때문에 3d context 를 최대한 활용할 수 있다.
    - [예시](https://jeeliz.com/demos/faceFilter/demos/threejs/dog_face/) 를 보면 고개를 좌우로 돌렸을때 강아지 필터가 입체감을 갖고 함께 움직인다.

#### tracking.js
- color tracking / face tracking / headtracking 세가지를 제공한다.
- face tracking 의 경우 트래킹할 대상에 대해 eye / face / mouth 옵션으로 선언할 수 있지만 정확도는 떨어진다.
    - 입벌리고 있으면 눈으로 인식한다 .. *
    - 3가지를 동시에 트래킹하면 매우 느리다.
- 사각형 범위로 추출 데이터를 리턴한다.

#### face-api.js `TODO`
- https://itnext.io/face-api-js-javascript-api-for-face-recognition-in-the-browser-with-tensorflow-js-bcc2a6c4cf07
- 텐서플로 라이브러리.......를 사용한다

#### headtrackr
- 얼굴영역에 대한 트래킹은 tracking.js 와 별 차이가 없다.
- head tracking 에 특화되어 있다.

#### +) face tracking vs head tracking
- face tracking 은 모델링 데이터를 기반으로 얼굴을 분석해서 눈코입을 찾는다.
    - clmtrackr 가 최고
    - 대부분의 라이브러리가 눈코입위치에 대한 box 데이터를 제공한다. 정수리까지 알수 있는 라이브러리가 왜 없을까. ..
- head tracking 은 카메라에서 부터 사용자 머리가 어디에 있는지 알 수 있는 기능이다.
    - 어느정도 떨어져 있는지, 어느 각도만큼 떨어져 있는지 알려주기 때문에 vr / ar 기술과 관련되어있다.
    - VR 기어 같은 디바이스에서는 자이로 스코프, 가속도계등이 사용되지만, 카메라에서는 유저의 시야각을 이용한다. (참고 [headtrackr overview](http://auduno.tumblr.com/post/25125149521/head-tracking-with-webrtc)) 



