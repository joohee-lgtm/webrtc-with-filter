## 실시간 캔버스 랜더링과 필터
- MediaStream 으로 비디오가 재생될때, 캔버스도 함께 업데이트 해준다.
- 디바이스 접근 이후에는 모든 것이 화면에 보여지는 canvas 를 중심으로 구현된다.
- 비디오를 매번 렌더링 해주면서 필터 렌더링도 함께 진행하면 requestFrameAnimation warning 을 자주 볼 수 있지만 ... 렌더링 성능의 개선 포인트로 삼아보도록 하자 ..

### `CanvasRenderingContext2D.filter` 로 색상 필터 적용하기
- 흑백처리, 블러, 대비 같은 이미지 효과를 줄 수 있다. 
- css 필터 속성과 동일한 값을 지정할 수 있다.
- svg 필터를 `url(#svg_id)` 와 같은 형태로 입력할 수 있다.
    - Sketch 에서는 작업물을 svg 코드로 복사할 수 있다. [참고](https://oss.navercorp.com/ApolloFE/realtime-filter/issues/3#issuecomment-2695255)
- 캔버스에서 제공하는 2d context API 를 사용하기 때문에 특정 부분에 대해서만 필터를 적용하거나, 텍스트를 추가하는 등의 추가적인 효과도 적용할 수 있다.
- 화면 중앙에 블러 효과를 추가한 코드 예시

<img width="300" alt="2018-11-24 9 49 08" src="https://media.oss.navercorp.com/user/237/files/9f3fbc46-f4c1-11e8-8dfa-af0d10587258">

```javascript
function render() {
  const bufferContext = buffer.getContext('2d');
  const outputContext =  output.getContext('2d');
  let width, height;

  // 거울모드
  width = buffer.width = video.videoWidth;
  heigth = buffer.height = video.videoHeight;
  bufferContext.translate(video.videoWidth, 0);
  bufferContext.scale(-1, 1);

  // 원본 스트림 렌더링 (배경)
  bufferContext.filter = ""
  bufferContext.drawImage(video, 0, 0);
  outputContext.drawImage(buffer,0,0);
  
  // 블러 영역 렌더링
  const x = buffer.width/2 - (buffer.width/4);
  const y = buffer.height/2 - (buffer.height/4);
  bufferContext.filter = "blur(20px)";
  bufferContext.drawImage(video, 0, 0);
  outputContext.drawImage(buffer, x, y, width/2, height/2, x, y, width/2, height/2);
  
  // 렌더링 완료 후 다음 프레임 렌더링
  window.requestAnimationFrame(render);
}
```

### webgl 필터 적용하기
- webgl 필터는 opengl 언어를 사용하는 shader 와 webgl 캔버스를 함께 사용함으로써 구현할 수 있다.
- 하지만 .. 쉐이더 코드를 잘 모를때는 gifx.js 과 같은 라이브러리를 사용해서 적용할 수 있다.
- gifx 는 webgl 컨텍스트 캔버스를 사용하기 때문에 필터 위에 바로 텍스트나 도형을 추가하는 2d context API 사용은 불가능하다.
   - webgl context 와 2d context 는 함께 사용이 불가능 하다.
   - 모든 캔버스는 첫번째 호출되는 컨텍스트로 캔버스의 컨텍스트가 고정된다.
- 필터가 씌워진 webgl context canvas 와 글씨나 도형을 (상대적으로) 쉽게 그릴 수 있는 2d context canvas 를 병합하여 사용할 수 있다.
- 아래는 webgl 과 shader 로 만들어진 gifx.js 라이브러리를 사용하여 필터를 적용하고 2d context fillText와 stroke API 를 이용해서 적용한 예시이다.

<img width="300" alt="webrtc" src="https://media.oss.navercorp.com/user/237/files/266aa23a-f4cc-11e8-9879-4ed91f9a3314">


```javascript
function render(){
    // webgl context <canvas> element
    // 일반 canvas 엘리먼트 아님, blur, draw 등의 라이브러리 API 가 바인딩 되어있다.
    const glfxCanvas = glfx.canvas(); 
    // MediaStream 이 재생되고 있는 <video> 를 webgl texture 로 사용
    const videoTexture = glfxCanvas.texture(video); 
    // glfx 라이브러리로 hexagon 효과 적용
    glfxCanvas.draw(videoTexture).hexagonalPixelate(320, 239.5, 20).update();
    // 2d 캔버스에 webgl 캔버스를 렌더링한다
    outputContext.drawImage(glfxCanvas, 0, 0);
    // 2d context api 사용하여 도형 추가
    const text = "WebGL & WebRTC";
    const textWidth = outputContext.measureText(text).width;
    ... style text 코드 생략
    outputContext.fillText(text,buffer.width/2,buffer.height/2);
    outputContext.beginPath();
    outputContext.moveTo(buffer.width/2-textWidth/2-30, buffer.height/2-50);
    ... lineTo 코드 생략
    outputContext.stroke();
    window.requestAnimationFrame(render);
}
```

### fabric.js 로 인터렉션 추가하기
- fabric.js 는 svg to canvas 파서를 내장한 대화형 객체모델을 제공하는 라이브러리이다.
- fabirc.js 에 추가되는 아이템은 기본적으로 click / drag and drop / resize / rotate 가 가능한 형태로 제공된다.
- 인터렉션을 사용하기 위해서는 Gesture 가 포함된 버전을 사용해야 한다. (http://fabricjs.com/build/)
- 캔버스에 스티커를 추가하고, 이동하고, 삭제하는 인터렉션을 구현하기 매우 쉽다.
- 아래는 스트리밍 화면에 이모지를 추가 / 삭제 하는 데모이다.
    - 2개의 캔버스에서 각각 1) fabric.js 로 추가한 아이템을 관리 / 2) video media stream 재생 의 역할을 한다.
    - 하나의 캔버스에서 모두 처리하면 비디오 프레임을 다시 그리고 난 후, fabric.js 에 의해 그려진 아이템들을 다시   복원해야 하는 처리가 추가된다.
    - CSS 로 두 캔버스의 사이즈를 동일하게 처리하고, 겹쳐두면 하나의 캔버스에서 동작하는것 처럼 보인다.
    - 이미지 또는 동영상으로 저장할때는 두개의 캔버스를 병합하여 사용한다.

<img width="300" alt="개발자 초상권 보호" src="https://media.oss.navercorp.com/user/237/files/44c74b50-f4d8-11e8-865c-77cbbdfdf8f3">

```html
<!-- 비디오 스트리밍 캔버스 -->
<canvas id="output">
<!-- fabric.js 생성영역 -->
<div id="canvas-container">
    <!-- 아이템 렌더링 -->
    <canvas id="fabric"></canvas>    
    <!-- 이벤트 바인딩 -->
    <canvas id="upper-canvas"></canvas>    
</div>
```

```javascript
fabricInstance.on({
    'mouse:down' : function(options) {
        // 클릭한 위치를 탐지 후 랜덤한 이모지 모양 출력
        !options.target && addRandomEmoji(options.pointer);
    },
    // 개별 아이템 드래그중
    'object:moving': function(e) {
        e.target.setCoords();
        fabricInstance.forEachObject(function(obj) {
            if (obj === e.target || obj.name !== "deleteIcon") return;
            // 화면에 그려진 이모지 아이템 중 삭제 아이콘과 겹쳐있는 아이콘이 있다면 삭제 아이콘 활성화
            obj.set('opacity' ,e.target.intersectsWithObject(obj) ? 0.8 : 0.4);
        });
    },
    // 개별 아이템 mouse up 시점
    'object:moved' : function(e) {
        e.target.setCoords();
        const deleteIcons = fabricInstance.getObjects().filter(function(obj) {
            return obj.name === "deleteIcon";
        });
        // 삭제 아이콘과 겹쳐있는 이모지 아이템은 삭제
        if(e.target.intersectsWithObject(deleteIcons[0])) {
            fabricInstance.remove(e.target);
        }
        // 삭제 아이콘 비활성화
        deleteIcons.forEach(function(icon) {
            icon.set("opacity", 0);
        })
    }
})
```


### face tracking 필터 적용하기
- face tracking API 는 라이브러리단의 모델링 데이터를 기반으로 얼굴을 분석한다.
- face tracking 데이터를 기반으로 head tracking 을 위한 유저의 시야각 데이터를 추출해낸다. 
    - 많은 라이브러리가 face tracking 과 head tracking 을 함께 제공한다.
- Readme 에 적혀있던 사용자에 강아지 귀가 달기는 face tracking 기술과 더 가깝기 때문에 head tracking 에 대해서는 테스트 하지 않았다.

> **head tracking?**
> - head tracking 은 디바이스로 부터 사용자 머리가 어느정도의 거리와 각도로 위치해있는지 추적하는 기술이다.
> - 주로 VR과 AR 기술에 사용되기 때문에 현재 사용자와 주변 환경을 하나로 통합한다는 기술적 측면에서 WebRTC와 더 밀접하다.
> - VR 기어 같은 디바이스에서는 자이로 스코프, 가속도계등이 사용되지만, 웹 카메라에서는 face tracking 에서 얻은 유저의 시야각을 이용한다.

#### clmtrackr
- 얼굴 각 요소에 대한 정확한 x,y 좌표를 배열형태로 리턴해준다.
- 가이드와 매핑되는 인덱스에서 각각의 얼굴 요소를 추출해서 사용할 수 있다.
- 분석이 완료될때마다 x,y 좌표가 존재하는 2차원 배열의 형태로 데이터를 리턴해준다

<img width="300" alt="2018-11-30 8 56 16" src="https://media.oss.navercorp.com/user/237/files/6b7472c8-f4e2-11e8-9b84-2a75830548ba">

```javascript 
// f : [[x1, y1], [x2, y2]...[x70, y70]]
function parseFaceModel(f) {
  const shape = f.slice(0, 15);
  const rightEyebrow = f.slice(15, 19);
  const leftEyebrow = f.slice(19, 23);
  // ... 생략

  return {
    shape, 
    rightEyePoint, leftEyePoint, 
    rightEye, leftEye, 
    rightEyebrow, leftEyebrow, 
    upperLip, lowerLip, 
    noseRidge, noseSide
  }
}
```
- 사각형 범위가 아닌, 정확한 shape 을 표현 할 수 있는 자료이기 때문에 눈코입에 어떤 모양을 매핑해야 할지 가장 정확하게 판단이 가능하다.
    - [표정 분석](https://www.auduno.com/clmtrackr/examples/clm_emotiondetection.html) : [상황별 데이터](https://github.com/auduno/clmtrackr/blob/dev/examples/js/emotionmodel.js)와 현재의 표정을 비교해서 화났는지, 슬픈지, 행복한지 구분해 낸다.
- 본인의 얼굴 모델을 빌드해서 라이브러리에서 활용할 수 있다.
- 한명만 인식한다.

#### jeelizFaceFilter
- getUserDevice 접근에 관한 제어도 모두 라이브러리에서 처리하고 있다.
- three.js helper 를 따로 제공한다. `THREE.JeelizHelper.init(spec, detect_callback);`
- face tracking  과 head tracking 에 대한 데이터가 동시에 제공되기 때문에 3d context 를 최대한 활용할 수 있다.
    - [예시](https://jeeliz.com/demos/faceFilter/demos/threejs/dog_face/) 를 보면 고개를 좌우로 돌렸을때 강아지 필터가 입체감을 갖고 함께 움직인다.

#### tracking.js
- color tracking / face tracking / headtracking 세가지를 제공한다.
- face tracking 의 경우 트래킹할 대상에 대해 eye / face / mouth 옵션으로 선언할 수 있지만 정확도는 떨어진다.
    - 입벌리고 있으면 눈으로 인식한다 .. *
    - 3가지를 동시에 트래킹하면 매우 느리다.
- 사각형 범위로 추출 데이터를 리턴한다.

#### face-api.js `TODO`
- https://itnext.io/face-api-js-javascript-api-for-face-recognition-in-the-browser-with-tensorflow-js-bcc2a6c4cf07
- 텐서플로 라이브러리.......를 사용한다

#### headtrackr
- 얼굴영역에 대한 트래킹은 tracking.js 와 별 차이가 없다.
- head tracking 에 특화되어 있다.


----
#### 참고자료
- https://dev.to/aspittel/facial-recognition-in-javascript-using-trackingjs-3l7
- http://auduno.tumblr.com/post/25125149521/head-tracking-with-webrtc
- https://ourcodeworld.com/articles/read/151/top-5-best-face-tracking-and-recognition-related-javascript-libraries